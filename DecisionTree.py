# -*- coding: utf-8 -*-
"""decision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/munawwar22HU/CS-351-AI-Project/blob/main/decision.ipynb
"""

## For reference, this is the goal to achieve
# df = pd.read_csv("data.csv")
# train_df, test_df = train_test_split(df, test_size_proportion=0.2)
# tree = decision_tree_algorithm(train_df)
# accuracy = calculate_accuracy(test_df, tree)

# Commented out IPython magic to ensure Python compatibility.
## Importing required modules
 
import numpy as np 
import pandas as pd
 
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
 
import random
#random.seed(0)
from pprint import pprint




"""
Train - Test - Split

"""

## Train-Test-Split
def train_test_split(df, test_size):

    # this checks whether the test_size is an integer or a proportion (float)
    # it is is a float, we convert it to an integer first
    if isinstance(test_size, float):
        test_size = round(test_size * len(df))

    indices = df.index.tolist()
    # print(indices)
    test_indices = random.sample(population=indices, k=test_size)
    test_df = df.loc[test_indices]  # this allows us to access only certain data rows (in this case the test_indices rows)
    train_df = df.drop(test_indices) # to remove the test dataframe from the training dataframe
    return train_df, test_df



""" Check Purity of Data"""

# this function will be used to check the purity of data
def check_purity(data):
    label_column = data[:, -1] # from the data 2d array, access only the last column (label) of every row
    unique_classes = np.unique(label_column) # from label_column, identify and choose all the unique columns
    # returns true if the data is pure, that means it contains only one class
    if len(unique_classes) == 1:
        return True
    # otherwise it returns false
    else: 
        return False

"""#Classify Data"""

def classify_data(data):
    label_column = data[:, -1]
    # this returns the unique classes along with the count of how many times each class appears in the data using return_counts
    classes, count_classes = np.unique(label_column, return_counts=True)

    index = count_classes.argmax()
    classification = classes[index]

    return classification

""" Get Potential Splits"""

### Potential Splits
def get_potential_splits(data):

    potential_splits = {}
    _, n_columns = data.shape
    for column_index in range(n_columns - 1):          # excluding the last column which is the label
        values = data[:, column_index]
        unique_values = np.unique(values)
        
        potential_splits[column_index] = unique_values
    
    return potential_splits


"""# Split Data"""

### Split data
def split_data(data, split_column, split_value):

    split_column_values = data[:,split_column]
    type_of_feature = feature_types[split_column]
    if type_of_feature == "continuous":
      data_below = data[split_column_values<= split_value]
      data_above = data[split_column_values>split_value]
    else:
      data_below = data[split_column_values == split_value]
      data_above = data[split_column_values !=split_value]
  
    return data_below, data_above



""" Lowest Entropy Overall"""

def calculate_entropy(data):
  label_column = data[:,-1]
  _, counts = np.unique (label_column,return_counts = True)
  #print(counts)
  probabilities = counts/counts.sum()
  #print(probabilities)
  entropy = -sum(probabilities *np.log2(probabilities))
  #print(entropy)
  return entropy

def calculate_overall_entropy(data_below,data_above):
    n_data_points = len(data_below)+len(data_above)
    p_data_below = len(data_below) / n_data_points
    p_data_above = len(data_above) / n_data_points

    overall_entropy = (p_data_below*calculate_entropy(data_below) + p_data_above * calculate_entropy(data_above))
    return overall_entropy


""" Lowest GINI Index value """

def calculate_gini(data):
  label_column = data[:,-1]
  _,counts = np.unique (label_column,return_counts = True)
  #print(counts)
  probabilities = counts/counts.sum()
  #print(probabilities)
  gini = 1 - sum(probabilities**2)
  #print(gini)
  return gini

def calculate_overall_gini(data_below,data_above):
    n_data_points = len(data_below)+len(data_above)
    p_data_below = len(data_below) / n_data_points
    p_data_above = len(data_above) / n_data_points

    overall_gini = (p_data_below*calculate_gini(data_below) + p_data_above * calculate_gini(data_above))


    return overall_gini


def determine_best_split(data,potential_splits,use_entropy = True):
  
  #potential_splits  = get_potential_splits(data)
  if (use_entropy == True):
    overall_entropy = 9999
    for column_index in potential_splits:
        for value in potential_splits[column_index]:
            data_below, data_above = split_data(data, split_column=column_index, split_value=value)
            current_overall_entropy = calculate_overall_entropy(data_below, data_above)
            
            if current_overall_entropy <=overall_entropy:
                overall_entropy = current_overall_entropy
                best_split_column = column_index
                best_split_value = value
  else:
    overall_gini = 9999
    for column_index in potential_splits:
      for value in potential_splits[column_index]:
          data_below, data_above = split_data(data, split_column=column_index, split_value=value)
          current_overall_gini = calculate_overall_gini(data_below, data_above)
          
          if current_overall_gini <=overall_gini:
              overall_gini = current_overall_gini
              best_split_column = column_index
              best_split_value = value


  return best_split_column,best_split_value

"""#Determine Type of Feature"""

def determine_type_of_feature(df):
  feature_types = []
  n_unique_values_threshold = 15
  for column in df.columns:
    
    unique_values = df[column].unique()
    example_Value = unique_values[0]
    if (isinstance(example_Value,str) or len(unique_values)<=n_unique_values_threshold):
      feature_types.append("categorical")
    else:
      feature_types.append("continuous")
  return feature_types



"""# Decision Tree Algorithm

"""

#sub_tree = {question : [yes_answer,no_answer]}

def decision_tree_algorithm(df,counter = 0,min_samples = 2,max_depth = 5,use_entropy = True ):
  if counter == 0:
    global column_headers,feature_types
    column_headers = df.columns
    feature_types = determine_type_of_feature(df)
    data = df.values
  else:
    data = df
  # base case 
  if (check_purity(data) or (len(data)<min_samples) or (counter == max_depth)):
    classification  = classify_data(data)
    return classification
  # recursive part
  else:
      counter +=1 
      potential_splits = get_potential_splits(data)
      split_column,split_value = determine_best_split(data,potential_splits,use_entropy)
      data_below, data_above = split_data(data, split_column,split_value)
      if len(data_below) == 0 or len(data_above) == 0:
            classification = classify_data(data)
            return classification
      # instantiate sub tree
      feature_name = column_headers[split_column]
      type_of_feature = feature_types[split_column]
      if type_of_feature == "continuous":
        question = "{} <= {}".format(feature_name,split_value)
      else:
        question = "{} = {}".format(feature_name,split_value)
      sub_tree = {question: []}

      # find answers 
      yes_answer  =  decision_tree_algorithm(data_below,counter,min_samples,max_depth)
      no_answer = decision_tree_algorithm(data_above,counter,min_samples,max_depth)

      if yes_answer == no_answer:
          sub_tree = yes_answer
      else:
        sub_tree[question].append(yes_answer)
        sub_tree[question].append(no_answer)

      return sub_tree

"""# Classification"""

def classify_example(example,tree):
  
  question = list(tree.keys())[0]
  feature_name,comparision_operator,value = question.split()
  # ask question
  if comparision_operator == "<=":
    if example[feature_name]<=float(value):
      answer =  tree[question][0]
    else:
      answer =  tree[question][1]
  else:
      if str(example[feature_name])==value:
        answer =  tree[question][0]
      else:
        answer =  tree[question][1]

  # Base Case
  if not isinstance(answer,dict):
    return answer
  # Recursive part
  else:
    residual_tree = answer
    return classify_example(example,residual_tree)

""" Accuracy """

def evaluate(df,tree):
  df['classification'] = df.apply(classify_example,axis=1,args=(tree,))
  df['classification_correct'] = df.classification == df.label
  classes = np.unique(df.label)
  n = len(classes)
  confusion_matrix = np.zeros((n,n))
  for i in range(n):
    for j in range(n):
      confusion_matrix[i][j]= sum(np.logical_and(df.label == classes[i],df.classification== classes[j]))
  print("Confusion Matrix ")
  pprint(confusion_matrix,width=50)
  accuracy = df.classification_correct.mean()
  precision = np.sum(np.diag(confusion_matrix)) / np.sum(np.sum(confusion_matrix, axis = 0))
  recall = np.diag(confusion_matrix)/ np.sum(confusion_matrix,axis=1)
  print("Accuracy : ",accuracy)
  print("Recall Score Classwise : " ,recall)
  print("Precision : ",precision)
  return accuracy,precision,recall

# random.seed(0)
# train_df,test_df = train_test_split(df,test_size=0.3)
# tree = decision_tree_algorithm(train_df,max_depth=3,use_entropy=False)
# evaluate(test_df,tree)
# pprint(tree,width=50)